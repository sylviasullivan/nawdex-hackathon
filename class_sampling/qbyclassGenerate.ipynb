{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from dict_nawdexsims import simdictionary\n",
    "import glob, sys, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up da dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out this dask thing from Aiko's GitHub repository :p\n",
    "from tempfile import NamedTemporaryFile, TemporaryDirectory # Creating temporary Files/Dirs\n",
    "import dask # Distributed data libary\n",
    "from dask_jobqueue import SLURMCluster # Setting up distributed memories via slurm\n",
    "from distributed import Client, progress, wait # Libaray to orchestrate distributed resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some user specific variables\n",
    "account_name = 'bb1018'\n",
    "partition = 'compute'\n",
    "job_name = 'cloud3d' # Job name that is submitted via sbatch\n",
    "memory = '64GiB' # Max memory per node that is going to be used - this depends on the partition\n",
    "cores = 48 # Max number of cores per that are reserved - also partition dependent\n",
    "walltime = '01:00:00' #'12:00:00' # Walltime - also partition dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_dir = '/scratch/b/b380873/' # Define the users scratch dir\n",
    "# Create a temp directory where the output of distributed cluster will be written to, after this notebook\n",
    "# is closed the temp directory will be closed\n",
    "dask_scratch_dir = TemporaryDirectory(dir=scratch_dir, prefix=job_name)\n",
    "cluster = SLURMCluster(memory=memory,\n",
    "                       cores=cores,\n",
    "                       project=account_name,\n",
    "                       walltime=walltime,\n",
    "                       queue=partition,\n",
    "                       name=job_name,\n",
    "                       processes=8,\n",
    "                       scheduler_options={'dashboard_address': ':12435'},\n",
    "                       local_directory=dask_scratch_dir.name,\n",
    "                       job_extra=[f'-J {job_name}', \n",
    "                                  f'-D {dask_scratch_dir.name}',\n",
    "                                  f'--begin=now',\n",
    "                                  f'--output={dask_scratch_dir.name}/LOG_cluster.%j.o',\n",
    "                                  f'--output={dask_scratch_dir.name}/LOG_cluster.%j.o'\n",
    "                                 ],\n",
    "                       interface='ib0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p compute\n",
      "#SBATCH -A bb1018\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=48\n",
      "#SBATCH --mem=64G\n",
      "#SBATCH -t 01:00:00\n",
      "#SBATCH -J cloud3d\n",
      "#SBATCH -D /scratch/b/b380873/cloud3d7mcu9u8d\n",
      "#SBATCH --begin=now\n",
      "#SBATCH --output=/scratch/b/b380873/cloud3d7mcu9u8d/LOG_cluster.%j.o\n",
      "#SBATCH --output=/scratch/b/b380873/cloud3d7mcu9u8d/LOG_cluster.%j.o\n",
      "\n",
      "JOB_ID=${SLURM_JOB_ID%;*}\n",
      "\n",
      "/pf/b/b380459/conda-envs/Nawdex-Hackathon/bin/python3 -m distributed.cli.dask_worker tcp://10.50.40.118:34779 --nthreads 6 --nprocs 8 --memory-limit 8.59GB --name name --nanny --death-timeout 60 --local-directory /scratch/b/b380873/cloud3d7mcu9u8d --interface ib0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad97585060ce45cca3bdc23c4845e2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>cloud3d</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .dataâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster.scale(jobs=1)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.50.40.118:34779</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.50.40.118:8787/status' target='_blank'>http://10.50.40.118:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.50.40.118:34779' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_client = Client(cluster)\n",
    "dask_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove first day from dataset\n",
    "def drop_first_day(ds):\n",
    "    ntime = ds.time.size                   # number of time steps\n",
    "    firstday = ds.isel(time=0).time.dt.day # first day \n",
    "    t_list = []                            # list of timesteps that do not belong to first day\n",
    "    for i in range(ntime):\n",
    "        if ds.isel(time=i).time.dt.day != firstday:\n",
    "            t_list.append(i)\n",
    "    return ds.isel(time=t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simdictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3294f4bba194>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Iterate through all simulations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbasedir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/work/bb1018/b380459/NAWDEX/ICON_OUTPUT_NWP/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msimdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimdictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Directories to be used below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'simdictionary' is not defined"
     ]
    }
   ],
   "source": [
    "# Iterate through all simulations\n",
    "basedir = '/work/bb1018/b380459/NAWDEX/ICON_OUTPUT_NWP/'\n",
    "simdict = simdictionary()\n",
    "\n",
    "# Directories to be used below\n",
    "classdir = '/pf/b/b380796/scratch/hackathon/george/'\n",
    "openoceandir = '/work/bb1018/nawdex-hackathon_pp/openoceanmask/'\n",
    "cloud3ddir = '/work/bb1018/b380459/NAWDEX/ICON_OUTPUT_NWP/'\n",
    "griddir = '/work/bb1018/icon_4_hackathon/grids/'\n",
    "griddict = {'80km':'R80000m','40km':'R40000m','20km':'R20000m','10km':'R10000m','5km':'R5000m','2km':'R2500m'}\n",
    "\n",
    "for s in list(simdict.keys())[:1]:\n",
    "    print(s)\n",
    "    \n",
    "    # Generate the file in which we'll save everything for this simulation.\n",
    "    ds = xr.Dataset({\n",
    "        \"qi\": ((\"lev\", \"classes\"), np.zeros((75,8))),\n",
    "        \"qc\": ((\"lev\", \"classes\"), np.zeros((75,8))),\n",
    "        \"qv\": ((\"lev\", \"classes\"), np.zeros((75,8)))\n",
    "        },\n",
    "        coords={\"lev\": np.arange(75),\"classes\": np.arange(1,9)},\n",
    "    )\n",
    "    \n",
    "    # Load the cloud classifications for this simulation.\n",
    "    classfi = classdir + 'nawdexnwp_' + simdict[s]['res'] + '_cloudclass_mis_' + s[-4:] + '.nc'\n",
    "    \n",
    "    # Take every second value from the classes as 3dcloud variables are only hourly.\n",
    "    print('Read cloud classification file')\n",
    "    classes = xr.open_dataset(classfi).drop({'lev','lev_2','lev_3','time'}).clch[::2]\n",
    "    print('Dimensions of classifications: ' + str(classes.shape))\n",
    "    \n",
    "    # Load the open ocean mask for this simulation and extract open ocean points.\n",
    "    print('Read ocean mask file')\n",
    "    oceanmask = xr.open_dataset(openoceandir + 'nawdexnwp-' + simdict[s]['res'] + '-mis-' + \n",
    "                                s[-4:] + '_openoceanmask.nc').mask_openocean.values\n",
    "    oo_idx = np.argwhere(oceanmask == 1)[:,0]\n",
    "    del oceanmask\n",
    "    \n",
    "    # Load the cloud3d values for this simulation.\n",
    "    print('Load cloud3d values')\n",
    "    cloud3d = xr.open_mfdataset(cloud3ddir + s + '/' + '*3dcloud*.nc',combine='by_coords',\n",
    "                               parallel=True, engine='h5netcdf', chunks={'time': 1})\n",
    "    \n",
    "    # Remove the first day from the values.\n",
    "    cloud3d = drop_first_day(cloud3d)\n",
    "    \n",
    "    # Extract the three cloud variables you need at the open ocean points.\n",
    "    qi = cloud3d.tot_qi_dia.isel(ncells=oo_idx)\n",
    "    qc = cloud3d.tot_qc_dia.isel(ncells=oo_idx)\n",
    "    qv = cloud3d.tot_qv_dia.isel(ncells=oo_idx)\n",
    "    \n",
    "    # Read in the grid file to area weight\n",
    "    print('Read grid file')\n",
    "    cell_area = xr.open_dataset(griddir + 'icon-grid_nawdex_78w40e23n80n_' + griddict[simdict[s]['res']] + '.nc')\n",
    "    cell_area = cell_area['cell_area'].rename({'cell': 'ncells'})\n",
    "    weights = cell_area / (cell_area).sum(dim=['ncells'])\n",
    "    weights = weights.isel(ncells=oo_idx)\n",
    "        \n",
    "    # Extract where the fields equal a certain cloud class and take a temporal mean.\n",
    "    print('Perform classification extraction and temporal mean')\n",
    "    qi_a = []; qc_a = []; qv_a = []\n",
    "    \n",
    "    for i in np.arange(1,9):\n",
    "        print(i)\n",
    "        qi = qi.where(classes==i).mean('time').mean('ncells')\n",
    "        qc = qc.where(classes==i).mean('time').mean('ncells')\n",
    "        qv = qv.where(classes==i).mean('time').mean('ncells')\n",
    "        \n",
    "        # Area weighting\n",
    "        #qi = (qi * weights).sum(dim='ncells')\n",
    "        #qc = (qc * weights).sum(dim='ncells')\n",
    "        #qv = (qv * weights).sum(dim='ncells')\n",
    "\n",
    "        #plt.plot(qi)\n",
    "        #qi_a.append(qi)\n",
    "        #qc_a.append(qc)\n",
    "        #qv_a.append(qv)\n",
    "        ds = xr.Dataset( data_vars=dict( qi=(\"lev\", qi), qc=(\"lev\", qc),\n",
    "            qv=(\"lev\", qv)))\n",
    "        ds.to_netcdf('q_mean-' + s + '_class' + str(i) + '.nc')\n",
    "    \n",
    "    #print('Update and save dataset')\n",
    "    #print(qi_a[0].values)\n",
    "    #print(qi_a[3].values)\n",
    "    #ds['qi'] = xr.concat(qi_a,\"classes\")\n",
    "    #ds['qc'] = xr.concat(qc_a,\"classes\")\n",
    "    #ds['qv'] = xr.concat(qv_a,\"classes\")\n",
    "    #ds.upate: (qii, 'qc': qcc, 'qv': qvv})\n",
    "    #ds.to_netcdf('q_mean-' + s + '.nc')\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " # CODE THAT DIDN'T WORK\n",
    "    # Which 3dcloud files correspond to this simulation? Order them too.\n",
    "    listing = sorted(glob.glob(basedir + s + '/*3dcloud*'))\n",
    "    \n",
    "    # Create a temporary array to hold the spatial-mean profiles for three variables \n",
    "    # over all timesteps.\n",
    "    \n",
    "    \n",
    "    temp = np.zeros((3, 75, len(listing)-24))\n",
    "    for j, file in enumerate(listing[24:]):\n",
    "        print(j)\n",
    "        # Extract the field of classifications for this time step.\n",
    "        classes_ts = classes[j]\n",
    "        \n",
    "        # Load the qi, qc, qv values for this simulation and for open-ocean grid cells.\n",
    "        qi = xr.open_dataset(file).tot_qi_dia.isel(time=0,ncells=oo_idx) # dims [=] (lev, ncells)\n",
    "        qc = xr.open_dataset(file).tot_qc_dia.isel(time=0,ncells=oo_idx)\n",
    "        qv = xr.open_dataset(file).tot_qv_dia.isel(time=0,ncells=oo_idx)\n",
    "           \n",
    "        # Filter out the values for each cloud class and take the spatial mean.\n",
    "        c = 1\n",
    "        temp[0, :, j-24] = qi.where(classes_ts == c).mean(dim={'ncells'})\n",
    "        temp[1, :, j-24] = qc.where(classes_ts == c).mean(dim={'ncells'})\n",
    "        temp[2, :, j-24] = qv.where(classes_ts == c).mean(dim={'ncells'})\n",
    "              \n",
    "    ds = xr.Dataset(\n",
    "            {\"qi\": ((\"lev\"), np.nanmean(temp[0], axis=2)),\n",
    "             \"qc\": ((\"lev\"), np.nanmean(temp[1], axis=2)),\n",
    "             \"qv\": ((\"lev\"), np.nanmean(temp[2], axis=2))},\n",
    "            coords={\"lev\": np.arange(75)},\n",
    "    )\n",
    "    ds.to_netcdf('q_mean-' + s + '_class' + str(c) + '.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nawdex-hackathon",
   "language": "python",
   "name": "nawdex-hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
