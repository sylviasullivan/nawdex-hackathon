{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Tue Jan 26 2021\n",
    "@author: Nicole Albern\n",
    "\n",
    "Plot vertical profiles of cloud heating rates for each\n",
    "cloud class.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib as mpl\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import shutil\n",
    "import zarr\n",
    "\n",
    "import sys\n",
    "sys.path.append('/pf/b/b380490/jupyter_notebooks/nawdex_hackathon')\n",
    "import dict_nawdexsims\n",
    "simdict = dict_nawdexsims.simdictionary()\n",
    "#colordict = dict_nawdexsims.colordictionary()\n",
    "\n",
    "from nawdexutils import drop_first_day, select_analysis_days\n",
    "\n",
    "#import dask\n",
    "#from dask.distributed import Client\n",
    "#client = Client()\n",
    "#client\n",
    "\n",
    "\n",
    "## remove first day from dataset\n",
    "#def drop_first_day(ds):\n",
    "#    ntime = ds.time.size                   # number of time steps\n",
    "#    firstday = ds.isel(time=0).time.dt.day # first day \n",
    "#    t_list = []                            # list of timesteps that do not belong to first day\n",
    "#    for i in range(ntime):\n",
    "#        if ds.isel(time=i).time.dt.day != firstday:\n",
    "#            t_list.append(i)\n",
    "#    return ds.isel(time=t_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out this dask thing from Aiko's GitHub repository :p\n",
    "from tempfile import NamedTemporaryFile, TemporaryDirectory # Creating temporary Files/Dirs\n",
    "import dask # Distributed data libary\n",
    "from dask_jobqueue import SLURMCluster # Setting up distributed memories via slurm\n",
    "from distributed import Client, progress, wait # Libaray to orchestrate distributed resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some user specific variables\n",
    "account_name = 'bb1018'\n",
    "partition = 'compute'\n",
    "job_name = 'cloud3d' # Job name that is submitted via sbatch\n",
    "memory = '64GiB' # Max memory per node that is going to be used - this depends on the partition\n",
    "cores = 48 # Max number of cores per that are reserved - also partition dependent\n",
    "walltime = '01:00:00' #'12:00:00' # Walltime - also partition dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pf/b/b380459/conda-envs/Nawdex-Hackathon/lib/python3.8/site-packages/distributed/node.py:151: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 45769 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scratch_dir = '/scratch/b/b380873/' # Define the users scratch dir\n",
    "# Create a temp directory where the output of distributed cluster will be written to, after this notebook\n",
    "# is closed the temp directory will be closed\n",
    "dask_scratch_dir = TemporaryDirectory(dir=scratch_dir, prefix=job_name)\n",
    "cluster = SLURMCluster(memory=memory,\n",
    "                       cores=cores,\n",
    "                       project=account_name,\n",
    "                       walltime=walltime,\n",
    "                       queue=partition,\n",
    "                       name=job_name,\n",
    "                       processes=8,\n",
    "                       scheduler_options={'dashboard_address': ':12435'},\n",
    "                       local_directory=dask_scratch_dir.name,\n",
    "                       job_extra=[f'-J {job_name}', \n",
    "                                  f'-D {dask_scratch_dir.name}',\n",
    "                                  f'--begin=now',\n",
    "                                  f'--output={dask_scratch_dir.name}/LOG_cluster.%j.o',\n",
    "                                  f'--output={dask_scratch_dir.name}/LOG_cluster.%j.o'\n",
    "                                 ],\n",
    "                       interface='ib0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p compute\n",
      "#SBATCH -A bb1018\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=48\n",
      "#SBATCH --mem=64G\n",
      "#SBATCH -t 01:00:00\n",
      "#SBATCH -J cloud3d\n",
      "#SBATCH -D /scratch/b/b380873/cloud3dwy_0h6ez\n",
      "#SBATCH --begin=now\n",
      "#SBATCH --output=/scratch/b/b380873/cloud3dwy_0h6ez/LOG_cluster.%j.o\n",
      "#SBATCH --output=/scratch/b/b380873/cloud3dwy_0h6ez/LOG_cluster.%j.o\n",
      "\n",
      "JOB_ID=${SLURM_JOB_ID%;*}\n",
      "\n",
      "/pf/b/b380459/conda-envs/Nawdex-Hackathon/bin/python3 -m distributed.cli.dask_worker tcp://10.50.40.118:40969 --nthreads 6 --nprocs 8 --memory-limit 8.59GB --name name --nanny --death-timeout 60 --local-directory /scratch/b/b380873/cloud3dwy_0h6ez --interface ib0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d37243310844974a81122660ff6709f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>cloud3d</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .dataâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster.scale(jobs=1)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.50.40.118:40969</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.50.40.118:45769/status' target='_blank'>http://10.50.40.118:45769/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.50.40.118:40969' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_client = Client(cluster)\n",
    "dask_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10km\n",
      "   0012\n",
      "    read q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pf/b/b380459/conda-envs/Nawdex-Hackathon/lib/python3.8/site-packages/xarray/core/indexing.py:1361: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   read cloud class\n",
      "    High (H)\n",
      "    Middle (M)\n",
      "    Low (L)\n",
      "    H-M\n",
      "    M-L\n",
      "    H-L\n",
      "    H-M-L\n",
      "    clear sky\n",
      "      save data\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#opath = '/scratch/b/b380490/hackathon/'\n",
    "opath = '/work/bb1018/b380873/hackathon/'\n",
    "\n",
    "ipath_grid = '/work/bb1018/icon_4_hackathon/'\n",
    "ipath_oceanmask = '/work/bb1018/nawdex-hackathon_pp/'\n",
    "#ipath_cloudheat = '/work/bb1018/nawdex-hackathon_pp/ddttemp_rad-from-fluxes/'\n",
    "ipath_cloudmask = '/pf/b/b380796/scratch/hackathon/george/'\n",
    "ipath_data = '/work/bb1018/b380459/NAWDEX/ICON_OUTPUT_NWP/'\n",
    "\n",
    "resolutions = ['10km']#['80km', '40km', '20km']#, '10km', '5km']#, '2km']\n",
    "#resolutions = ['2km', '2km-shcon']\n",
    "gridres = ['R10000m']#['R80000m', 'R40000m', 'R20000m', 'R10000m', 'R5000m']#, 'R2500m']\n",
    "#gridres = ['R2500m', 'R2500m']\n",
    "\n",
    "# cloud classes\n",
    "ncclass = 8 # number of cloud classes\n",
    "cclasses = ['High (H)', 'Middle (M)', 'Low (L)',\n",
    "            'H-M', 'M-L', 'H-L', 'H-M-L', 'clear sky']\n",
    "cclass_save = ['H', 'M', 'L', 'H-M', 'M-L', 'H-L', 'H-M-L', 'clearsky']\n",
    "\n",
    "for r, resolution in enumerate(resolutions):\n",
    "    print(resolution)\n",
    "    if resolution in ['80km', '40km', '20km']:\n",
    "        sims = [ '0001', '0002', '0003', '0004', '0005',\n",
    "                '0006', '0007', '0008', '0009', '0010']\n",
    "    elif resolution == '2km-shcon':\n",
    "        sims = ['0001-shcon', '0002-shcon', '0005-shcon',\n",
    "                '0006-shcon', '0007-shcon', '0008-shcon',\n",
    "                '0009-shcon', '0010-shcon', '0011-shcon',\n",
    "                '0012-shcon']\n",
    "        resolution = '2km'\n",
    "    else:\n",
    "        sims = ['0012']#['0001', '0002', '0003', '0004', '0005',\n",
    "                #'0006', '0007', '0008', '0009', '0010', '0011', '0012']\n",
    "        \n",
    "    for sim in sims:\n",
    "        print('  ', sim)\n",
    "        expid = 'nawdexnwp-' + resolution + '-mis-' + sim\n",
    "\n",
    "        ##########################################################################\n",
    "        # read ocean mask\n",
    "        da_ocean = xr.open_dataset(ipath_oceanmask + '/openoceanmask/' + expid + \\\n",
    "                                   '_openoceanmask.nc')['mask_openocean']\n",
    "        index = np.where(da_ocean == 1)[0]\n",
    "        del da_ocean\n",
    "\n",
    "        ##########################################################################\n",
    "        # read cell area\n",
    "        da_cell_area = xr.open_dataset(ipath_grid + '/grids/icon-grid_nawdex_78w40e23n80n_' + \\\n",
    "                                       gridres[r] + '.nc')['cell_area'].rename({'cell': 'ncells'})\n",
    "\n",
    "        # weight for area mean\n",
    "        weights = da_cell_area / (da_cell_area).sum(dim=['ncells'])\n",
    "\n",
    "        # apply open ocean mask\n",
    "        weights = weights.isel(ncells=index)\n",
    "\n",
    "        del da_cell_area\n",
    "\n",
    "        ##########################################################################\n",
    "        # read cloud cover\n",
    "        #print('   read cloud cover')\n",
    "        print('    read q')\n",
    "        da_ccover = xr.open_mfdataset(ipath_data + expid + '/' +\n",
    "                                      expid + '_2016*_3dcloud_DOM01_ML_*.nc',\n",
    "                                      combine='by_coords',parallel=True,\n",
    "                                      engine='h5netcdf', chunks={'time': 1})['tot_qv_dia']\n",
    "        \n",
    "        #da_ccover = xr.open_mfdataset(ipath_data + expid + '/' +\n",
    "        #                              expid + '_2016*_3dcloud_DOM01_ML_*.nc',\n",
    "        #                              combine='by_coords',parallel=True,\n",
    "        #                              engine='h5netcdf', chunks={'time': 1})\n",
    "        #print(da_ccover.shape)\n",
    "        \n",
    "        ## remove first day from dataset\n",
    "        #da_ccover = drop_first_day(da_ccover)\n",
    "        #print(da_ccover.shape)\n",
    "        \n",
    "        # get days that are available for all resolutions\n",
    "        da_ccover = select_analysis_days(da_ccover, expid)\n",
    "        \n",
    "        # apply open ocean mask\n",
    "        da_ccover = da_ccover.isel(ncells=index)\n",
    "        \n",
    "        #print(da_ccover.shape)\n",
    "        \n",
    "        #print(da_ccover.values.shape)\n",
    "        #print(da_ccover.time.values[0:6])\n",
    "        #print(da_ccover.time.values[-6:])\n",
    "        \n",
    "        ##########################################################################\n",
    "        # read cloud classes\n",
    "        print('   read cloud class')\n",
    "        # read cloud class: \n",
    "        # cloud class stored every 30 minutes; cloud cover stored every hour\n",
    "        # -> resample cloud class to get it every hour\n",
    "        da_cclass = xr.open_dataset(ipath_cloudmask + 'nawdexnwp_' + resolution + \\\n",
    "                                    '_cloudclass_mis_' + sim + '_hq65_mq70_lq35.nc').rename({'clch': 'cclass'}).resample(time=\"1H\").nearest(tolerance=\"5M\")['cclass']\n",
    "  \n",
    "        # get days that are available for all resolutions\n",
    "        da_cclass = select_analysis_days(da_cclass, expid)\n",
    "        \n",
    "        #print('cloud class shape and time')\n",
    "        #print(da_cclass.values.shape)\n",
    "        #print(da_cclass.time.values[0:6])\n",
    "        #print(da_cclass.time.values[-6:])\n",
    "        \n",
    "        ##########################################################################\n",
    "        # calculate time mean and area mean cloud cover for cloud classes\n",
    "        for c, clas in enumerate(cclasses):\n",
    "            print('   ', clas)\n",
    "\n",
    "            # time mean\n",
    "            da_mean = da_ccover.where(da_cclass == c+1).mean('time')\n",
    "            \n",
    "            # weighted area mean\n",
    "            da_mean = (da_mean*weights).sum(dim='ncells')\n",
    "            \n",
    "            # store data in dataset            \n",
    "            if c == 0:\n",
    "                ds_meanall = da_mean.to_dataset(name='cclass' + str(c+1))\n",
    "                #ds_meanall.name = 'cloud3d_cclass'\n",
    "            else:\n",
    "                ds_meanall = ds_meanall.update(da_mean.to_dataset(name='cclass' + str(c+1)))\n",
    "                #ds_meanall.name = 'cloud3d_cclass'\n",
    "            \n",
    "            del da_mean\n",
    "        del c, clas\n",
    "        \n",
    "        # save means at nc file\n",
    "        print('      save data')\n",
    "        ofile = expid + '_qv_hq65_mq70_lq35.nc'\n",
    "        ds_meanall.to_netcdf(path=opath + ofile, format='NETCDF3_64BIT')#,engine='netcdf4')#, unlimited_dims='time')\n",
    "        #ofile = expid + '_qc_hq60_mq60_lq25'\n",
    "        #ds_meanall.to_zarr(store=opath + ofile)\n",
    "        \n",
    "        del ds_meanall, ofile\n",
    "    \n",
    "        ##########################################################################\n",
    "        del index, weights\n",
    "        del expid\n",
    "        del da_cclass, da_ccover\n",
    "    del sim\n",
    "\n",
    "del resolution\n",
    "print('Done!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''############################################'''\n",
    "'''          !!!!! OLD VERSION !!!!!           '''\n",
    "'''store each cloud class in an individual file'''\n",
    "'''############################################'''\n",
    "\n",
    "##############################################################################\n",
    "opath = '/scratch/b/b380490/hackathon/'\n",
    "\n",
    "ipath_grid = '/work/bb1018/icon_4_hackathon/'\n",
    "ipath_oceanmask = '/work/bb1018/nawdex-hackathon_pp/'\n",
    "#ipath_cloudheat = '/work/bb1018/nawdex-hackathon_pp/ddttemp_rad-from-fluxes/'\n",
    "ipath_cloudmask = '/pf/b/b380796/scratch/hackathon/george/'\n",
    "ipath_data = '/work/bb1018/b380459/NAWDEX/ICON_OUTPUT_NWP/'\n",
    "\n",
    "#resolutions = ['80km', '40km', '20km']#, '10km', '5km', '2km']\n",
    "resolutions = ['10km', '5km']#, '2km']\n",
    "#gridres = ['R80000m', 'R40000m', 'R20000m']\n",
    "gridres = ['R10000m', 'R5000m']#, 'R2500m']\n",
    "\n",
    "\n",
    "# cloud classes\n",
    "ncclass = 8 # number of cloud classes\n",
    "cclasses = ['High (H)', 'Middle (M)', 'Low (L)',\n",
    "            'H-M', 'M-L', 'H-L', 'H-M-L', 'clear sky']\n",
    "cclass_save = ['H', 'M', 'L', 'H-M', 'M-L', 'H-L', 'H-M-L', 'clearsky']\n",
    "\n",
    "for r, resolution in enumerate(resolutions):\n",
    "    print(resolution)\n",
    "    if resolution in ['80km', '40km', '20km']:\n",
    "        sims = ['0001', '0002', '0003', '0004', '0005',\n",
    "                '0006', '0007', '0008', '0009', '0010']\n",
    "    else:\n",
    "        sims = ['0001', '0002', '0003', '0004', '0005',\n",
    "                '0006', '0007', '0008', '0009', '0010', '0011', '0012']\n",
    "        \n",
    "    for sim in sims:\n",
    "        print('  ', sim)\n",
    "        expid = 'nawdexnwp-' + resolution + '-mis-' + sim\n",
    "\n",
    "        ##########################################################################\n",
    "        # read ocean mask\n",
    "        da_ocean = xr.open_dataset(ipath_oceanmask + '/openoceanmask/' + expid + \\\n",
    "                                   '_openoceanmask.nc')['mask_openocean']\n",
    "        index = np.where(da_ocean == 1)[0]\n",
    "        del da_ocean\n",
    "\n",
    "        ##########################################################################\n",
    "        # read cell area\n",
    "        da_cell_area = xr.open_dataset(ipath_grid + '/grids/icon-grid_nawdex_78w40e23n80n_' + \\\n",
    "                                       gridres[r] + '.nc')['cell_area'].rename({'cell': 'ncells'})\n",
    "\n",
    "        # weight for area mean\n",
    "        weights = da_cell_area / (da_cell_area).sum(dim=['ncells'])\n",
    "\n",
    "        # apply open ocean mask\n",
    "        weights = weights.isel(ncells=index)\n",
    "\n",
    "        del da_cell_area\n",
    "\n",
    "        ##########################################################################\n",
    "        # read cloud cover\n",
    "        print('   read cloud cover')\n",
    "        da_ccover = xr.open_mfdataset(ipath_data + expid + '/' +\n",
    "                                      expid + '_2016*_3dcloud_DOM01_ML_*.nc',\n",
    "                                      combine='by_coords',parallel=True,\n",
    "                                      engine='h5netcdf', chunks={'time': 1})['clc']\n",
    "        #print(da_ccover.shape)\n",
    "        \n",
    "        # remove first day from dataset\n",
    "        da_ccover = drop_first_day(da_ccover)\n",
    "        #print(da_ccover.shape)\n",
    "        \n",
    "        # apply open ocean mask\n",
    "        da_ccover = da_ccover.isel(ncells=index)\n",
    "        #print(da_ccover.shape)\n",
    "        \n",
    "        #print(da_ccover.values.shape)\n",
    "        #print(da_ccover.time.values[0:6])\n",
    "        #print(da_ccover.time.values[-6:])\n",
    "        \n",
    "        ##########################################################################\n",
    "        # read cloud classes\n",
    "        print('   read cloud class')\n",
    "        ds_cc = xr.open_dataset(ipath_cloudmask + 'nawdexnwp_' + resolution + \\\n",
    "                                '_cloudclass_mis_' + sim + '.nc').rename({'clch': 'cclass'})\n",
    "                            # old data:\n",
    "                                #'cloud_class_array_thres10p_' + \\\n",
    "                                #resolution + '_alltimesteps_v3.nc').rename({'clch': 'cclass'})\n",
    "\n",
    "        # cloud class stored every 30 minutes. heating rates stored every hour\n",
    "        # store cloud class also at every hour\n",
    "        da_cclass = ds_cc.cclass[::2]\n",
    "        del ds_cc\n",
    "\n",
    "        #print('cloud class shape and time')\n",
    "        #print(da_cclass.values.shape)\n",
    "        #print(da_cclass.time.values[0:6])\n",
    "        #print(da_cclass.time.values[-6:])\n",
    "        \n",
    "        ##########################################################################\n",
    "        # calculate time mean and area mean cloud-radiative heating rates for\n",
    "        # cloud classes\n",
    "        #means = {}\n",
    "        #means.fromkeys(cclasses)\n",
    "        for c, clas in enumerate(cclasses):\n",
    "            print('   ', clas)\n",
    "\n",
    "            # time mean\n",
    "            da_mean = da_ccover.where(da_cclass == c+1).mean('time')\n",
    "            \n",
    "            # weighted area mean\n",
    "            da_mean = (da_mean*weights).sum(dim='ncells')\n",
    "            \n",
    "            #print(da_mean.values.min(), da_mean.values.max())\n",
    "            #print(da_mean.values)\n",
    "            \n",
    "            # save means as nc file\n",
    "            print('      save data')\n",
    "            ofile = expid + '_' + cclass_save[c] + \\\n",
    "                    '_cloudcover_timemean_areamean_oceanmask_applied.nc'\n",
    "            da_mean.to_netcdf(path=opath + ofile, format='NETCDF3_64BIT')#, unlimited_dims='time')\n",
    "            \n",
    "            del da_mean, ofile\n",
    "        del c, clas\n",
    "        ##########################################################################\n",
    "        del index, weights\n",
    "        del expid\n",
    "        del da_cclass, da_ccover\n",
    "    del sim\n",
    "\n",
    "del resolution\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncplot",
   "language": "python",
   "name": "ncplot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
