{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud vertical classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that classifies the vertical cloud cover structure based on high-mid-low cloud cover and return a mask with the cloud classes.\n",
    "\n",
    "input: \n",
    "    -Model output of high-, mid- and low-level cloud cover\n",
    "    -Provide a cloud cover threshold to clasify a layer as cloudy or not\n",
    "    \n",
    "output:\n",
    "    -1-D matrix of the cloud class for each column\n",
    "    \n",
    "The 1-D matrix will consist of 8 classes as follows:\n",
    "        CL1: H          CL2: M          CL3: L          CL4: HM         CL5: ML         CL6: HL        CL7: HML        CL8: clear-sky       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cartopy import crs as ccrs\n",
    "from matplotlib import pyplot as plt\n",
    "import psutil\n",
    "\n",
    "from tempfile import NamedTemporaryFile, TemporaryDirectory # Creating temporary Files/Dirs\n",
    "import dask # Distributed data libary\n",
    "from dask_jobqueue import SLURMCluster # Setting up distributed memories via slurm\n",
    "from distributed import Client, progress, wait # Libaray to orchestrate distributed resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some user specific variables\n",
    "account_name = 'bb1018'\n",
    "partition = 'compute'\n",
    "job_name = 'nawdexProc_george' # Job name that is submitted via sbatch\n",
    "memory = '64GiB' # Max memory per node that is going to be used - this depends on the partition\n",
    "cores = 48 # Max number of cores per that are reserved - also partition dependent\n",
    "walltime = '01:00:00' #'12:00:00' # Walltime - also partition dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pf/b/b380459/conda-envs/Nawdex-Hackathon/lib/python3.8/site-packages/distributed/node.py:151: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 33133 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scratch_dir = '/scratch/b/b380796/' # Define the users scratch dir\n",
    "# Create a temp directory where the output of distributed cluster will be written to, after this notebook\n",
    "# is closed the temp directory will be closed\n",
    "dask_scratch_dir = TemporaryDirectory(dir=scratch_dir, prefix=job_name)\n",
    "cluster = SLURMCluster(memory=memory,\n",
    "                       cores=cores,\n",
    "                       project=account_name,\n",
    "                       walltime=walltime,\n",
    "                       queue=partition,\n",
    "                       name=job_name,\n",
    "                       processes=8,\n",
    "                       scheduler_options={'dashboard_address': ':12435'},\n",
    "                       local_directory=dask_scratch_dir.name,\n",
    "                       job_extra=[f'-J {job_name}', \n",
    "                                  f'-D {dask_scratch_dir.name}',\n",
    "                                  f'--begin=now',\n",
    "                                  f'--output={dask_scratch_dir.name}/LOG_cluster.%j.o',\n",
    "                                  f'--output={dask_scratch_dir.name}/LOG_cluster.%j.o'\n",
    "                                 ],\n",
    "                       interface='ib0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p compute\n",
      "#SBATCH -A bb1018\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=48\n",
      "#SBATCH --mem=64G\n",
      "#SBATCH -t 01:00:00\n",
      "#SBATCH -J nawdexProc_george\n",
      "#SBATCH -D /scratch/b/b380796/nawdexProc_georgeamhh_9ra\n",
      "#SBATCH --begin=now\n",
      "#SBATCH --output=/scratch/b/b380796/nawdexProc_georgeamhh_9ra/LOG_cluster.%j.o\n",
      "#SBATCH --output=/scratch/b/b380796/nawdexProc_georgeamhh_9ra/LOG_cluster.%j.o\n",
      "\n",
      "JOB_ID=${SLURM_JOB_ID%;*}\n",
      "\n",
      "/pf/b/b380459/conda-envs/Nawdex-Hackathon/bin/python3 -m distributed.cli.dask_worker tcp://10.50.39.196:45585 --nthreads 6 --nprocs 8 --memory-limit 8.59GB --name name --nanny --death-timeout 60 --local-directory /scratch/b/b380796/nawdexProc_georgeamhh_9ra --interface ib0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5692790f6ad4b6d8408afbf3e133c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>nawdexProc_george</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster.scale(jobs=1)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST          START_TIME  NODES SCHEDNODES           NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "! squeue -u $USER --start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.50.39.93:35144</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.50.39.93:8787/status' target='_blank'>http://10.50.39.93:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.50.39.93:35144' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_client = Client(cluster)\n",
    "dask_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iconsimulation(expid,gridres):\n",
    "    \n",
    "    print('Working on loading data for', expid)\n",
    "    path  = '/scratch/b/b380459/icon_4_hackathon/'\n",
    "    \n",
    "    # 2d datasets\n",
    "    fname = path+'/'+expid+'/'+expid+'_2016*_2d_30min_DOM01_ML_*.nc'\n",
    "    print(fname)\n",
    "    ds_2d_30min = ( xr.open_mfdataset(fname,\n",
    "                                      combine='by_coords',parallel=True, \n",
    "                                      engine='h5netcdf', chunks={'time': 1})\n",
    "                    [['clct', 'tqv_dia', 'tqc_dia', 'tqi_dia']] )\n",
    "    \n",
    "    # grid dataset\n",
    "    fname = path+'/grids/icon-grid_nawdex_78w40e23n80n_'+gridres+'.nc'\n",
    "    ds_grid = ( xr.open_dataset(fname)\n",
    "               [['cell_area','clat','clon','clon_vertices','clat_vertices',\n",
    "                 'vlon','vlat','vertex_of_cell', 'vertex']].rename({'cell': 'ncells'}) ) \n",
    "    # we need to subtract -1 from vertex_of_cell as python starts counting at 0, but fortran starts at 1\n",
    "    ds_grid['vertex_of_cell'] = ds_grid['vertex_of_cell'] - 1 \n",
    "   \n",
    "    # get land-sea mask from extpar data\n",
    "    fname = path+'/extpar/extpar_icon-grid_nawdex_78w40e23n80n_'+gridres+'_bitmap.nc'\n",
    "    ds_extp = xr.open_dataset(fname)[['FR_LAND']].rename({'cell': 'ncells'})\n",
    "    \n",
    "    # merge datasets\n",
    "    ds = xr.merge([ds_2d_30min, ds_grid, ds_extp])\n",
    "        \n",
    "    # convert grid from radians to degrees\n",
    "    ds['clon'] = np.rad2deg(ds['clon'])\n",
    "    ds['clat'] = np.rad2deg(ds['clat'])\n",
    "    ds['vlon'] = np.rad2deg(ds['vlon'])\n",
    "    ds['vlat'] = np.rad2deg(ds['vlat'])\n",
    "    ds['clon_vertices'] = np.rad2deg(ds['clon_vertices'])\n",
    "    ds['clat_vertices'] = np.rad2deg(ds['clat_vertices'])\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory = 856.0 MB\n"
     ]
    }
   ],
   "source": [
    "# function that prints out the memory used\n",
    "def print_memory(msg=None):\n",
    "    process = psutil.Process()\n",
    "    if (msg):\n",
    "        print(msg, ':', 'memory =', np.round(process.memory_info().rss/(1024*1024)), 'MB')\n",
    "    else:\n",
    "        print('memory =', np.round(process.memory_info().rss/(1024*1024)), 'MB')\n",
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on loading data for nawdexnwp-80km-mis-0001\n",
      "/scratch/b/b380459/icon_4_hackathon//nawdexnwp-80km-mis-0001/nawdexnwp-80km-mis-0001_2016*_2d_30min_DOM01_ML_*.nc\n"
     ]
    }
   ],
   "source": [
    "ds = load_iconsimulation('nawdexnwp-80km-mis-0001','R80000m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tricontourf(ds.clon, ds.clat, ds['clct'].isel(time=80));\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory = 855.0 MB\n"
     ]
    }
   ],
   "source": [
    "# we load all the timesteps for the 80km resolution\n",
    "ipath='/scratch/b/b380459/icon_4_hackathon/nawdexnwp-80km-mis-0001/'\n",
    "ifile = 'nawdexnwp-80km-mis-0001_2016092200_2d_30min_*.nc'\n",
    "\n",
    "#ipath='/scratch/b/b380459/icon_4_hackathon/nawdexnwp-40km-mis-0001/'\n",
    "#ifile = 'nawdexnwp-40km-mis-0001_2016092200_2d_30min_*.nc'\n",
    "\n",
    "#ipath='/scratch/b/b380459/icon_4_hackathon/nawdexnwp-20km-mis-0001/'\n",
    "#ifile = 'nawdexnwp-20km-mis-0001_2016092200_2d_30min_*.nc'\n",
    "\n",
    "#ipath='/scratch/b/b380459/icon_4_hackathon/nawdexnwp-10km-mis-0001/'\n",
    "#ifile = 'nawdexnwp-10km-mis-0001_2016092200_2d_30min_*.nc'\n",
    "\n",
    "#ipath='/scratch/b/b380459/icon_4_hackathon/nawdexnwp-5km-mis-0001/'\n",
    "#ifile = 'nawdexnwp-5km-mis-0001_2016092200_2d_30min_*.nc'\n",
    "\n",
    "#ipath='/scratch/b/b380459/icon_4_hackathon/nawdexnwp-2km-mis-0001/'\n",
    "#ifile = 'nawdexnwp-2km-mis-0001_2016092200_2d_30min_*.nc'\n",
    "\n",
    "data = xr.open_mfdataset(ipath+ifile)\n",
    "data\n",
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data = data.sel(time='2016-09-22T18:00:00.000000000')\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the land areas and the domain boundaries using the openoceanmask \n",
    "\n",
    "def load_openoceanmask(expid):\n",
    "    path  = '/scratch/b/b380459/icon_4_hackathon/'\n",
    "    fname = path+'/openocean_masks/'+expid+'_openoceanmask.nc'\n",
    "    return xr.open_dataset(fname)['mask_openocean']\n",
    "\n",
    "da_oom  = load_openoceanmask('nawdexnwp-80km-mis-0001')\n",
    "#da_oom  = load_openoceanmask('nawdexnwp-40km-mis-0001')\n",
    "#da_oom  = load_openoceanmask('nawdexnwp-20km-mis-0001')\n",
    "#da_oom  = load_openoceanmask('nawdexnwp-10km-mis-0001')\n",
    "#da_oom  = load_openoceanmask('nawdexnwp-5km-mis-0001')\n",
    "#da_oom  = load_openoceanmask('nawdexnwp-2km-mis-0001')\n",
    "\n",
    "\n",
    "index = np.where(da_oom==1)[0]\n",
    "data = data.isel(ncells=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set a threshold for the cloud cover to identify cloudy and clear sky grid boxes\n",
    "thres=10\n",
    "\n",
    "# function that gets 2-D cloud cover data of High, Mid and Low level cloud cover and creates classes for the vertical column\n",
    "def cloud_class(cc_data,thres):\n",
    "    # CL1: H \n",
    "    # CL2: M \n",
    "    # CL3: L \n",
    "    # CL4: HM \n",
    "    # CL5: ML \n",
    "    # CL6: HL \n",
    "    # CL7: HML \n",
    "    # CL8: clear-sky\n",
    "    \n",
    "    # Create the vertical cloud classes based on the threshold (thres)\n",
    "    cl1     = cc_data.where((cc_data.clch>thres) & (cc_data.clcm<thres) & (cc_data.clcl<thres)) # H\n",
    "    cl1_num = cl1.where(xr.ufuncs.isnan(cl1.clch),other=1)\n",
    "    cl1_num = cl1_num.where((xr.ufuncs.isnan(cl1_num.clch))==False,other=0)\n",
    "\n",
    "    cl2 = cc_data.where((cc_data.clch<thres) & (cc_data.clcm>thres) & (cc_data.clcl<thres)) # M\n",
    "    cl2_num = cl2.where(xr.ufuncs.isnan(cl2.clch),other=2)\n",
    "    cl2_num = cl2_num.where((xr.ufuncs.isnan(cl2_num.clch))==False,other=0)\n",
    "\n",
    "    \n",
    "    cl3 = cc_data.where((cc_data.clch<thres) & (cc_data.clcm<thres) & (cc_data.clcl>thres)) # L\n",
    "    cl3_num = cl3.where(xr.ufuncs.isnan(cl3.clch),other=3)\n",
    "    cl3_num = cl3_num.where((xr.ufuncs.isnan(cl3_num.clch))==False,other=0)\n",
    "\n",
    "    \n",
    "    cl4 = cc_data.where((cc_data.clch>thres) & (cc_data.clcm>thres) & (cc_data.clcl<thres)) # HM\n",
    "    cl4_num = cl4.where(xr.ufuncs.isnan(cl4.clch),other=4)\n",
    "    cl4_num = cl4_num.where((xr.ufuncs.isnan(cl4_num.clch))==False,other=0)\n",
    "\n",
    "    \n",
    "    cl5 = cc_data.where((cc_data.clch<thres) & (cc_data.clcm>thres) & (cc_data.clcl>thres)) # ML\n",
    "    cl5_num = cl5.where(xr.ufuncs.isnan(cl5.clch),other=5)\n",
    "    cl5_num = cl5_num.where((xr.ufuncs.isnan(cl5_num.clch))==False,other=0)\n",
    "\n",
    "\n",
    "    cl6 = cc_data.where((cc_data.clch>thres) & (cc_data.clcm<thres) & (cc_data.clcl>thres)) # HL\n",
    "    cl6_num = cl6.where(xr.ufuncs.isnan(cl6.clch),other=6)\n",
    "    cl6_num = cl6_num.where((xr.ufuncs.isnan(cl6_num.clch))==False,other=0)\n",
    "\n",
    "    \n",
    "    cl7 = cc_data.where((cc_data.clch>thres) & (cc_data.clcm>thres) & (cc_data.clcl>thres)) # HML\n",
    "    cl7_num = cl7.where(xr.ufuncs.isnan(cl7.clch),other=7)\n",
    "    cl7_num = cl7_num.where((xr.ufuncs.isnan(cl7_num.clch))==False,other=0)\n",
    "\n",
    "    \n",
    "    cl8 = cc_data.where((cc_data.clch<thres) & (cc_data.clcm<thres) & (cc_data.clcl<thres)) # clear-sky\n",
    "    cl8_num = cl8.where(xr.ufuncs.isnan(cl8.clch),other=8)\n",
    "    cl8_num = cl8_num.where((xr.ufuncs.isnan(cl8_num.clch))==False,other=0)\n",
    "\n",
    "    # we sum all the classes to one array to make a mask for the entire domain based on the cloud classification\n",
    "    cloud_class_mask   = cl1+cl2+cl3+cl4+cl5+cl6+cl7+cl8 \n",
    "    cloud_class_number = cl1_num+cl2_num+cl3_num+cl4_num+cl5_num+cl6_num+cl7_num+cl8_num\n",
    "    return cloud_class_mask, cloud_class_number\n",
    "\n",
    "cloud_class_array, cloud_class_number = cloud_class(data,thres)\n",
    "\n",
    "print('###################################################################')\n",
    "print('TEST OUTPUT')\n",
    "print('HIGH',cloud_class_array.clch.sel(ncells=slice(4642,4660)).squeeze().values)\n",
    "print('MID' ,cloud_class_array.clcm.sel(ncells=slice(4642,4660)).squeeze().values)\n",
    "print('LOW' ,cloud_class_array.clcl.sel(ncells=slice(4642,4660)).squeeze().values)\n",
    "\n",
    "print('HIGH',cloud_class_number.clch.sel(ncells=slice(4642,4660)).squeeze().values)\n",
    "print('MID' ,cloud_class_number.clcm.sel(ncells=slice(4642,4660)).squeeze().values)\n",
    "print('LOW' ,cloud_class_number.clcl.sel(ncells=slice(4642,4660)).squeeze().values)\n",
    "print('###################################################################')\n",
    "\n",
    "#cloud_class_number\n",
    "\n",
    "#print('I save the cloud class number')\n",
    "#opath= '/pf/b/b380796/scratch/hackathon/george/'\n",
    "#np.save(opath+'cloud_class_array_thres10p_2km_alltimesteps_v2',cloud_class_number.squeeze().values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncplot",
   "language": "python",
   "name": "ncplot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
