{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain mean of different datasets for NAWDEX simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing utility dictionaries\n",
    "import sys\n",
    "sys.path.append('/pf/b/b381185/behrooz/Hackathon_b/nawdex-hackathon/shared')\n",
    "\n",
    "# simulations dictionary\n",
    "import dict_nawdexsims\n",
    "simdict = dict_nawdexsims.simdictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud radiative heating rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radiative Heating rates + cloud. Function for Loading, masking, domain_mean, and saving to nc file \n",
    "\n",
    "def load_iconnwp_hr_data():\n",
    "    \n",
    "    ds_list = []\n",
    "    \n",
    "    # looping through simulations\n",
    "    for sim in list(simdict.keys()):\n",
    "        \n",
    "        print('Working on loading data for', sim)\n",
    "        path = '/work/bb1018/nawdex-hackathon_pp/ddttemp_rad-from-fluxes/'\n",
    "        \n",
    "        #print(path+'/'+sim+'/'+sim+'_ddttemp_rad-from-fluxes_DOM01_ML.zarr')\n",
    "        # loading radiative heating rate datasets\n",
    "        ds = xr.open_zarr(path+sim+'_ddttemp_rad-from-fluxes_DOM01_ML.zarr')#,chunks={'time': 1})\n",
    "        \n",
    "        # Loading open_ocean mask datasets\n",
    "        fname = '/work/bb1018/nawdex-hackathon_pp/openoceanmask/'+sim+'_openoceanmask.nc'\n",
    "        ds_om = xr.open_dataset(fname)['mask_openocean']\n",
    "        index = np.where(ds_om==1)[0]\n",
    "        # Ocean masking\n",
    "        ds = ds.isel(ncells=index)\n",
    "        \n",
    "        # loading related grid files for domain mean calculations   \n",
    "        if simdict[sim]['res'] == '80km':\n",
    "            fname1 = '/work/bb1018/icon_4_hackathon/grids/icon-grid_nawdex_78w40e23n80n_'+grids[0]+'.nc'\n",
    "        elif simdict[sim]['res'] == '40km':\n",
    "            fname1 = '/work/bb1018/icon_4_hackathon/grids/icon-grid_nawdex_78w40e23n80n_'+grids[1]+'.nc'\n",
    "        elif simdict[sim]['res'] == '20km':\n",
    "            fname1 = '/work/bb1018/icon_4_hackathon/grids/icon-grid_nawdex_78w40e23n80n_'+grids[2]+'.nc'\n",
    "        elif simdict[sim]['res'] == '10km':\n",
    "            fname1 = '/work/bb1018/icon_4_hackathon/grids/icon-grid_nawdex_78w40e23n80n_'+grids[3]+'.nc'\n",
    "        elif simdict[sim]['res'] == '5km':\n",
    "            fname1 = '/work/bb1018/icon_4_hackathon/grids/icon-grid_nawdex_78w40e23n80n_'+grids[4]+'.nc'\n",
    "        elif simdict[sim]['res'] == '2km':\n",
    "            fname1 = '/work/bb1018/icon_4_hackathon/grids/icon-grid_nawdex_78w40e23n80n_'+grids[5]+'.nc'\n",
    "        \n",
    "        dg = ( xr.open_dataset(fname1)[['cell_area']].rename({'cell': 'ncells'}))\n",
    "        # open_ocean mascking for grid files \n",
    "        dg = dg.isel(ncells=index)\n",
    "            \n",
    "        # Domain mean\n",
    "        weights=dg['cell_area']/(dg['cell_area']).sum(dim=['ncells'])\n",
    "        ds = (ds*weights).sum(dim=['ncells'])\n",
    "        #########\n",
    "        # Saving to nc files    \n",
    "        ds.attrs['simulation'] = sim\n",
    "        \n",
    "        ds.to_netcdf('/work/bb1018/nawdex-hackathon_pp/radiative_heating_domain_mean/atmradheating_fldmean_openocean_'+sim+'.nc')\n",
    "\n",
    "    return ds_list\n",
    "\n",
    "#---------------------------------------\n",
    "ds_icon_list_hr = load_iconnwp_hr_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables. Loading, masking, domain_mean (looping through time steps), and saving to nc file \n",
    "\n",
    "# Settings for running this function: Compute 64GB, 24 cores, 1 node, whit dask configuration\n",
    "# Multiple files, time step looping\n",
    "\n",
    "def load_iconnwp_data():\n",
    "    \n",
    "    ds_list = []\n",
    "    \n",
    "    # looping through simulations\n",
    "    for sim in list(simdict.keys()):\n",
    "        \n",
    "        print('Working on loading data for', sim)\n",
    "        path = '/work/bb1018/b380459/NAWDEX/ICON_OUTPUT_NWP/'\n",
    "        \n",
    "        # Create a list to extract time steps from datasets\n",
    "        tstep_list = []\n",
    "        \n",
    "        for i in Path('/work/bb1018/b380459/NAWDEX/ICON_OUTPUT_NWP/'+sim).rglob(sim+'_2016*_2d_30min_DOM01_ML_*.nc'): \n",
    "            tstep_list.append(str(i).split('/')[-1].split('_')[6].split('.')[0]) # 1 hourly -> split('_')[5] \n",
    "            tstep_list.sort(key=int)\n",
    "        \n",
    "        # loading open_ocean masking datasets\n",
    "        fname = '/work/bb1018/nawdex-hackathon_pp/openoceanmask/'+sim+'_openoceanmask.nc' #(.split('-shcon')[-2])change only for shcon sims\n",
    "        ds_om = xr.open_dataset(fname)['mask_openocean']\n",
    "        index = np.where(ds_om==1)[0]\n",
    "        \n",
    "        # loading related grid files for domain mean calculations  \n",
    "        \n",
    "        if simdict[sim]['res'] == '80km':\n",
    "            fname1 = '/work/bb1018/icon_4_hackathon/grids/icon-grid_nawdex_78w40e23n80n_'+grids[0]+'.nc'\n",
    "        elif simdict[sim]['res'] == '40km':\n",
    "            fname1 = '/work/bb1018/icon_4_hackathon/grids/icon-grid_nawdex_78w40e23n80n_'+grids[1]+'.nc'\n",
    "        elif simdict[sim]['res'] == '20km':\n",
    "            fname1 = '/work/bb1018/icon_4_hackathon/grids/icon-grid_nawdex_78w40e23n80n_'+grids[2]+'.nc'\n",
    "        elif simdict[sim]['res'] == '10km':\n",
    "            fname1 = '/work/bb1018/icon_4_hackathon/grids/icon-grid_nawdex_78w40e23n80n_'+grids[3]+'.nc'\n",
    "        elif simdict[sim]['res'] == '5km':\n",
    "            fname1 = '/work/bb1018/icon_4_hackathon/grids/icon-grid_nawdex_78w40e23n80n_'+grids[4]+'.nc'\n",
    "        elif simdict[sim]['res'] == '2km':\n",
    "            fname1 = '/work/bb1018/icon_4_hackathon/grids/icon-grid_nawdex_78w40e23n80n_'+grids[5]+'.nc'\n",
    "        \n",
    "            \n",
    "        dg = ( xr.open_dataset(fname1)[['cell_area']].rename({'cell': 'ncells'}))\n",
    "            \n",
    "        # open_ocean mascking grid files\n",
    "        dg = dg.isel(ncells=index)\n",
    "        \n",
    "        # Create a list to concat datasets with time dimension\n",
    "        ds_t=[]\n",
    "        # looping through time step for loading datasets\n",
    "        for tstep in tstep_list:\n",
    "            \n",
    "            # choose datasets\n",
    "            \n",
    "            #ds = xr.open_mfdataset(path+'/'+sim+'/'+sim+'_2016*_3dcloud_DOM01_ML_'+str(tstep).zfill(4)+'.nc',\n",
    "            #                    combine='by_coords', parallel=True, \n",
    "            #                    engine='h5netcdf', chunks={'ncells': 1e6} )[['clc']] #'time': 1 #change datasets\n",
    "            \n",
    "            #------------------------------------------------------------------------------------\n",
    "            #ds = xr.open_mfdataset(path+'/'+sim+'/'+sim+'_2016*_2drad_30min_DOM01_ML_'+str(tstep).zfill(4)+'.nc',\n",
    "            #                    combine='by_coords', parallel=True, \n",
    "            #                    engine='h5netcdf', chunks={'ncells': 1e6})\n",
    "            \n",
    "            # derive net shortwave fluxes at TOA\n",
    "            #ds['sob_t'] = (ds['sod_t']-ds['sou_t'])\n",
    "        \n",
    "            # derive TOA CRE fluxes\n",
    "            #ds['swtoacre'] = ds['sob_t'] - ds['swtoaclr']\n",
    "            #ds['lwtoacre'] = ds['thb_t'] - ds['lwtoaclr']\n",
    "            #ds['nttoacre'] = ds['lwtoacre'] + ds['swtoacre']\n",
    "        \n",
    "            # derive Surface CRE fluxes\n",
    "            #ds['swsfccre'] = ds['sob_s'] - ds['swsfcclr']\n",
    "            #ds['lwsfccre'] = ds['thb_s'] - ds['lwsfcclr']\n",
    "            #ds['ntsfccre'] = ds['swsfccre'] + ds['lwsfccre']\n",
    "        \n",
    "            # derive atmospheric CRE fluxes\n",
    "            #ds['swatmcre'] = ds['swtoacre'] - ds['swsfccre']\n",
    "            #ds['lwatmcre'] = ds['lwtoacre'] - ds['lwsfccre']\n",
    "            #ds['ntatmcre'] = ds['nttoacre'] - ds['ntsfccre']\n",
    "            \n",
    "            #--------------------------------------------------------------------------------\n",
    "            \n",
    "            #ds = xr.open_mfdataset(path+'/'+sim+'/'+sim+'_2016*_ddttemp_DOM01_ML_'+str(tstep).zfill(4)+'.nc',\n",
    "            #                    combine='by_coords', parallel=True, \n",
    "            #                    engine='h5netcdf', chunks={'ncells': 1e6}) [['ddt_temp_dyn2','ddt_temp_radlw','ddt_temp_radsw','ddt_temp_totnwpphy','ddt_temp_mphy','ddt_temp_turb']]\n",
    "            \n",
    "            #--------------------------------------------------------------------------------\n",
    "            #ds = xr.open_mfdataset(path+'/'+sim+'/'+sim+'_2016*_2d_30min_DOM01_ML_'+str(tstep).zfill(4)+'.nc',\n",
    "            #                    combine='by_coords', parallel=True, \n",
    "            #                    engine='h5netcdf', chunks={'ncells': 1e6} )[['tqc_dia','tqi_dia','clch','clcm','clcl','clct','tqc','tqi']]\n",
    "        \n",
    "            ds = xr.open_mfdataset(path+'/'+sim+'/'+sim+'_2016*_2d_30min_DOM01_ML_'+str(tstep).zfill(4)+'.nc',\n",
    "                                combine='by_coords', parallel=True, \n",
    "                                engine='h5netcdf', chunks={'ncells': 1e6} )[['snow_con_rate','rain_con_rate','snow_gsp_rate','rain_gsp_rate',\n",
    "                                                                             'graupel_gsp_rate','tot_prec']]\n",
    "            #--------------------------------------------------------------------------------\n",
    "            \n",
    "            # open_ocean masking dataset\n",
    "            ds = ds.isel(ncells=index)\n",
    "            \n",
    "            # Domain mean\n",
    "            weights=dg['cell_area']/(dg['cell_area']).sum(dim=['ncells'])\n",
    "            ds = (ds*weights).sum(dim=['ncells']).compute()\n",
    "\n",
    "            # Cancating time steps into 1 dataset for each simulation\n",
    "            ds_t.append(ds)\n",
    "            ds_c = xr.concat(ds_t, dim=\"time\")\n",
    "            \n",
    "        # Saving to nc files\n",
    "        ds_c.attrs['simulation'] = sim\n",
    "    \n",
    "        # change the path based on datasets\n",
    "        ds_c.to_netcdf('/work/bb1018/nawdex-hackathon_pp/2d_30min_domain_mean/rain_snow/2d_30min_'+sim+'.nc')\n",
    "        \n",
    "        del ds, ds_om, index, dg, ds_t ,tstep_list, ds_c\n",
    "            \n",
    "    return ds_list\n",
    "#-----------------------------------\n",
    "ds_icon_list = load_iconnwp_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncplot",
   "language": "python",
   "name": "ncplot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
